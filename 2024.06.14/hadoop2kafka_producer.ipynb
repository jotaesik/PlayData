pip install confluent_kafka
from confluent_kafka import Producer #카프카접속할때
from pyarrow import fs #하둡접속할때
import configparser, os ,subprocess #standard때문에

from confluent_kafka import Producer
from pyarrow import fs
import configparser, os, subprocess

class Hdfs2Kafka(object):
    def __init__(self):
        classpath = subprocess.Popen(["/home/hadoop/hadoop/bin/hdfs", "classpath", "--glob"],	#새로운 프로레스 생성
                         stdout=subprocess.PIPE).communicate()[0]	#표준출력을 다시 프로세스로 보내고 표준출력만을 가져오기 원래는 표준출력,표준에러
        os.environ['CLASSPATH'] = classpath.decode('utf-8')	 #자바의 클래스파일을 찾는 경로 지정 
        os.environ['ARROW_LIBHDFS_DIR'] = "/home/hadoop/hadoop/lib/native"
        self._hdfs = fs.HadoopFileSystem(host='ip', port=port, user='hadoop')

        kafka_brokers = "ip:port"
        kafka_resetType = 'earliest'	#earlist or last가 있는데 last는 가장 최근의 오프셋 처음부터 읽는것이 아니다.

        conf = {'bootstrap.servers' : kafka_brokers, 'auto.offset.reset' : kafka_resetType}
        self._producer = Producer(conf)

    def getHdFileInfo(self, filename):
        f_Info = self._hdfs.get_file_info(filename)	#file인지 directory인지 구별
        print(f'파일 종류 : {str(f_Info.type)}')
        print(f'파일 경로 : {str(f_Info.path)}')
        print(f'파일 크기 : {str(f_Info.size)}')
        print(f'파일 수정일자 : {str(f_Info.mtime)}')

    def readHdFile(self, filename):
        with self._hdfs.open_input_file(filename) as f:
            read_data = f.read().decode('utf-8').splitlines()
            #tmp = []
            #for line in read_data:
            #    tmp.append(line.split(","))	#금융권은 ,로 금액을 분류한다
            #return tmp
            return [line.split(",") for line in read_data]

    def sendData2Kafka(self, topic, list_line):
        for data in list_line:
            str_tmp = ",".join(data).split(",")
            modified_data = ",".join(str_tmp[:2]) + "," + ",".join(str_tmp[4:])
            print(modified_data)
            self._producer.poll(0)
            self._producer.produce(topic, value=modified_data.encode('utf-8'), 
                                callback = kafka_producer_call)
            self._producer.flush()

def kafka_producer_call(err, msg):
    if err is not None:
        print (f"실패한 메시지 : error={err}, message={msg}")
    else:
        print(f"전달한 메시지: {msg.topic()} [{msg.partition()}] @ {msg.offset()}")
        print(f"massge.topic = {msg.topic()}")
        print(f"massge.timestamp = {msg.timestamp()}")
        print(f"massge.key = {msg.key()}")
        print(f"massge.value = {msg.value().decode('utf-8')}")
        print(f"massge.partition = {msg.partition()}")
        print(f"massge.offset = {msg.offset()}")


from confluent_kafka import Producer
from pyarrow import fs
import configparser, os, subprocess

class Hdfs2Kafka(object):
    def __init__(self):
        classpath = subprocess.Popen(["/home/hadoop/hadoop/bin/hdfs", "classpath", "--glob"],
                         stdout=subprocess.PIPE).communicate()[0]
        os.environ['CLASSPATH'] = classpath.decode('utf-8')
        os.environ['ARROW_LIBHDFS_DIR'] = "/home/hadoop/hadoop/lib/native"
        self._hdfs = fs.HadoopFileSystem(host='ip', port=port, user='hadoop')

        kafka_brokers = "ip:port"
        kafka_resetType = 'earliest'

        conf = {'bootstrap.servers' : kafka_brokers, 'auto.offset.reset' : kafka_resetType}
        self._producer = Producer(conf)

    def getHdFileInfo(self, filename):
        f_Info = self._hdfs.get_file_info(filename)
        print(f'파일 종류 : {str(f_Info.type)}')
        print(f'파일 경로 : {str(f_Info.path)}')
        print(f'파일 크기 : {str(f_Info.size)}')
        print(f'파일 수정일자 : {str(f_Info.mtime)}')

    def readHdFile(self, filename):
        with self._hdfs.open_input_file(filename) as f:
            read_data = f.read().decode('utf-8').splitlines()
            #tmp = []
            #for line in read_data:
            #    tmp.append(line.split(","))
            #return tmp
            return [line.split(",") for line in read_data]

    def sendData2Kafka(self, topic, list_line):
        for data in list_line:
            str_tmp = ",".join(data).split(",")
            modified_data = ",".join(str_tmp[:2]) + "," + ",".join(str_tmp[4:])
            print(modified_data)
            self._producer.poll(0)
            self._producer.produce(topic, value=modified_data.encode('utf-8'), 
                                callback = kafka_producer_call)
            self._producer.flush()

def kafka_producer_call(err, msg):
    if err is not None:
        print (f"실패한 메시지 : error={err}, message={msg}")
    else:
        print(f"전달한 메시지: {msg.topic()} [{msg.partition()}] @ {msg.offset()}")
        print(f"massge.topic = {msg.topic()}")
        print(f"massge.timestamp = {msg.timestamp()}")
        print(f"massge.key = {msg.key()}")
        print(f"massge.value = {msg.value().decode('utf-8')}")
        print(f"massge.partition = {msg.partition()}")
        print(f"massge.offset = {msg.offset()}")


test01 = Hdfs2Kafka()

test01.getHdFileInfo("/mort/hadoop_data/civilian_force.csv")

temp = test01.readHdFile("/mort/hadoop_data/civilian_force.csv")

test01.sendData2Kafka("encore_mort", temp)
