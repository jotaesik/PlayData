{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef559f5f-6dd3-4b92-bf27-62f40aadb0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 00:01:55,796 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# 2024.06.26 어제꺼\n",
    "import os\n",
    "import subprocess\n",
    "from pyarrow import fs\n",
    "import pyarrow as pa\n",
    "\n",
    "classpath = subprocess.Popen([\"/home/hadoop/hadoop/bin/hdfs\", \"classpath\", \"--glob\"], stdout=subprocess.PIPE).communicate()[0]\n",
    "os.environ[\"CLASSPATH\"] = classpath.decode(\"utf-8\")\n",
    "hdfs = fs.HadoopFileSystem(host='ip', port=port, user='hadoop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5f75c4d-a248-4ed8-9cf5-7c01f187ba13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 01:14:17 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://producer:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8e1a9b5ed0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"bsh\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480e49d7-271f-4f64-89a2-10ac4c700425",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/'\n",
    "\n",
    "# 디렉토리 목록 가져오기\n",
    "file_infos = hdfs.get_file_info(pa.fs.FileSelector(dir_path, recursive=False))  \n",
    "for file_info in file_infos:\n",
    "    # print(file_info.path)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ee4378-17e9-4f19-9372-f78059589c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 00:01:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"20240626\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"hdfs:////encore/tpss_bcycl_od_statnhm_202001.csv\", encoding='cp949', header=True)\n",
    "\n",
    "\n",
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1297e576-eb5e-4f82-9964-cfdc9d943942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1543"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "test = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df.select(\"시작_대여소ID\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "964267fe-e4d4-458a-ab1a-2ee69c8da890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cf7cb5d-4058-4a2a-bc1e-b9da6442978a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|avg(전체이용시간(분))|\n",
      "+---------------------+\n",
      "|   19.249510036830372|\n",
      "+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"전체이용거리(m)\", col(\"전체이용거리(m)\").cast('int'))\n",
    "from pyspark.sql.types import IntegerType\n",
    "df3 = df.withColumn(\"전체이용거리(m)\", col(\"전체이용거리(m)\").cast(IntegerType()))\n",
    "df2.withColumn(\"전체이용시간(분)\", col(\"전체이용시간(분)\").cast('int')).select(mean(col(\"전체이용시간(분)\"))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c17fb019-e8dc-46d8-841a-585ff573e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7f30f86-4fb7-4176-94e8-feb20c93cbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|stddev_samp(전체이용시간(분))|\n",
      "+-----------------------------+\n",
      "|            26.96801174397804|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|avg(전체이용시간(분))|\n",
      "+---------------------+\n",
      "|   19.249510036830372|\n",
      "+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = StructType().\\\n",
    "      add('기준_날짜',  StringType(), True ).\\\n",
    "        add('구분코드',  StringType(), True ).\\\n",
    "        add('기준_시간',  StringType(), True ).\\\n",
    "        add('시작_대여소ID',  StringType(), True ).\\\n",
    "        add('시작_대여소명',  StringType(), True ).\\\n",
    "        add('종료_대여소ID',  StringType(), True ).\\\n",
    "        add('종료_대여소명',  StringType(), True ).\\\n",
    "        add('전체건수',  IntegerType(), True ).\\\n",
    "        add('전체이용시간(분)',  IntegerType(), True ).\\\n",
    "        add('전체이용거리(m)' , IntegerType(), True )\n",
    "\n",
    "tmp = spark.read.csv(\"hdfs:////encore/tpss_bcycl_od_statnhm_202001.csv\", encoding='cp949', header=True, schema=schema)\n",
    "\n",
    "from pyspark.sql.functions import mean, col, max, min, stddev\n",
    "\n",
    "tmp.select( stddev('전체이용시간(분)')).show()\n",
    "\n",
    "\n",
    "tmp.select( mean('전체이용시간(분)')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "487830db-c8b7-4030-b3ba-85654a573414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hdfs dfs -get /encore/tpss_bcycl_od_statnhm_202001.csv ./\n",
    "# !hdfs dfs -get /encore/tpss_bcycl_od_statnhm_202007.csv ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94df3b-6ce0-4efe-9abd-3b977cb3ec75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2176b1b-264f-45bf-b724-446bb574bb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23482792-e829-4767-a66f-ca5f1f50ffef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1669003"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "tmp.select(coalesce('전체이용시간(분)')).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a0ba1f3-4738-4582-8501-4aabfc66f4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|null|null|\n",
      "|   1|null|\n",
      "|null|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Returns the first column that is not null.\n",
    "\n",
    "\n",
    "cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
    "cDf.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cDf = spark.createDataFrame([(None, None, None), (None, 1, None), (None, 2, 6), (6, 3, 4)], (\"a\", \"b\", \"c\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a36a656-b616-4f3e-bfa3-40aca3dc1ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|   a|   b|   c|\n",
      "+----+----+----+\n",
      "|null|null|null|\n",
      "|null|   1|null|\n",
      "|null|   2|   6|\n",
      "|   6|   3|   4|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85cc268e-5a4a-4182-8671-a3c7fb920235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "166a8553-98dd-4734-b7a4-f35ec3db67f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|(a IS NULL)|\n",
      "+-----------+\n",
      "|       true|\n",
      "|       true|\n",
      "|       true|\n",
      "|      false|\n",
      "+-----------+\n",
      "\n",
      "+----+----+----+\n",
      "|null|null|null|\n",
      "+----+----+----+\n",
      "|   3|   1|   2|\n",
      "+----+----+----+\n",
      "\n",
      "+--------+-----+-------+\n",
      "|category|value|new_col|\n",
      "+--------+-----+-------+\n",
      "|       A|   10|    low|\n",
      "|       B|   20|    low|\n",
      "|       C|   30|   high|\n",
      "+--------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cDf.select(isnull(\"a\")).show()\n",
    "\n",
    "\n",
    "cDf.filter(isnull(\"a\") | isnull(\"b\")).count()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, when, count\n",
    "cDf.agg(*[count(when(isnull(x), x)).\\\n",
    "          alias(\"null\") \n",
    "             for x in cDf.columns]).show()\n",
    "\n",
    "when_test = spark.createDataFrame([(\"A\", 10), (\"B\", 20), (\"C\", 30)], [\"category\", \"value\"])\n",
    "\n",
    "df_new = when_test.withColumn(\"new_col\", when(col(\"value\") > 20, \"high\").otherwise(\"low\"))\n",
    "df_new.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a49209e-5e03-48bd-8460-538b4b1cc8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------------+-------------+-------------+-------------+--------+----------------+---------------+\n",
      "|기준_날짜|구분코드|기준_시간|시작_대여소ID|시작_대여소명|종료_대여소ID|종료_대여소명|전체건수|전체이용시간(분)|전체이용거리(m)|\n",
      "+---------+--------+---------+-------------+-------------+-------------+-------------+--------+----------------+---------------+\n",
      "|        0|       0|        0|            0|            0|            0|            0|       0|               0|              0|\n",
      "+---------+--------+---------+-------------+-------------+-------------+-------------+--------+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tmp.agg(*[count(when(isnull(x), x)).\\\n",
    "          alias(x) \n",
    "             for x in tmp.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5d36329-95ce-448a-9606-2a93051c75b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get: `./tpss_bcycl_od_statnhm_202001.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -get /encore/tpss_bcycl_od_statnhm_202001.csv ./\n",
    "!hdfs dfs -get /encore/tpss_bcycl_od_statnhm_202007.csv ./"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
