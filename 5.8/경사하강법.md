- 경사하강법(Gradient Decent)은 함수의 값이 낮아지는 방향으로 독립변수의 값을 변경하면서 최종적으로 최소 함숫값을 갖도록 하는 독립 변숫값을 찾는 방식 
- 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 최적화 알고리즘으로서, 함수를 최소화하기 위해 파라미터를 반복적으로 조정해나감 
- 머신러닝에서 함수의 최솟값을 찾는 방식으로 경사하강법을 사용하는 이유는 크게 두 가지로 설명할 수 있음 
	1. 분석에서 마주하는 함수들은 형태가 복잡해 수식으로 미분계수와 그 해를 찾기 어려울 수 있다 
	2. 데이터의 양이 큰 경우 경사하강법이 상대적으로 쉽게 컴퓨터로 구현될 수 있다 
- 초기에 무작위초기화(Random Initialization) 방식을 통해 임의의 값으로 시작해 한 번에 조금씩 함수의 값이 감소하는 방향으로 최솟값에 수렴할 때까지 점진적으로 진행 
- 이때 학습 스텝의 크기를 학습률(Learning Rate)라고 함  
	- 학습률이 너무 작으면 수렴할때까지 반복을 여러 번 수행해야 하므로 시간이 오래 걸림
	- 반대로 학습률이 너무 높이면 함수의 값이 발산되는 경향이 있음 
	- 적정한 크기로 조절해야 함 
# 배치 경사하강법
- 반복 시 전체 훈련세트를 사용해 가중값을 갱신 
- 특징 
	- 계산량이 많아 훈련에 소요되는 시간이 증가 
	- 하지만 학습 시 발생하는 잡음이 적은 최적치를 찾을 수 있음 

# 확률적 경사하강법
- 한 개의 샘플 데이터를 무작위로 선택하고 그 샘플에 대한 경사를 계산 
- 매 반복마다 가중값이 달라지기 때문에 비용 함수가 최솟값에 접근할 때 확률값으로 요동치며 평균적으로 감소함 
- 특징 
	- 최솟값이 요동치면서 접근하기 때문에 알고리즘이 멈출 때 최적치가 아닐 때가 있음 
	- 하지만 지역 최솟값을 건너뛰고 전역 최솟값으로 다다를 가능성이 높고 데이터의 양이 많아도 계산 속도가 빠름 

# 미니 배치 경사하강법 
- 각 스텝을 반복할 때 임의의 30~50개 관측값으로 경사를 계산하고 모델의 가중값을 갱신 
- 특징 
	- 파라미터공산에서 확률적 경사하강법보다 지역 최솟값에서 빠져나오기 어려울 가능성이 있음 
	- 하지만 확률적 경사하강법보다 낮은 오차율을 가지게 되어 최솟값에 더 가까이 도달할 수 있음 

# SGDRegressor
- scikit-learn 패키지의 경사하강법을 활용한 선형 회귀 모델 
- 확률적 경사하강법을 사용한 방식으로 회귀 모델을 구현 
- 손실의 기울기는 각 샘플에서 한 번에 추정되며 모델은 감소하는 강도, 즉 학습률에 따라 업데이트 됨 
- 정규화는 L1, L2 또는 이 둘의 조합인 엘라스틱넷을 이용해 매개변수를 0벡터로 축소하는 손실 함수에 추가된 패널티 